{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6de4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from skimage.io import imread\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import train_test_split\n",
    "import requests\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.losses import SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "import cv2\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import to_categorical\n",
    "import requests\n",
    "import xml.etree.ElementTree as ET\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f073048",
   "metadata": {},
   "source": [
    "# Creating Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6443006",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_fasta_data(url):\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        return response.text\n",
    "    else:\n",
    "        print(f\"Failed to retrieve data: {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "def parse_fasta(data):\n",
    "    protein_ids = []\n",
    "    for line in data.split('\\n'):\n",
    "        if line.startswith('>'):\n",
    "            # extract the protein ID from the header \n",
    "            protein_id = line.split('|')[1]  \n",
    "            protein_ids.append(protein_id)\n",
    "    return protein_ids\n",
    "\n",
    "url = 'https://rest.uniprot.org/uniprotkb/stream?format=fasta&query=%28organism_id%3A9606%29+AND+%28reviewed%3Atrue%29+AND+%28proteins_with%3A27%29+AND+%28proteins_with%3A7%29+AND+%28proteins_with%3A57%29'\n",
    "fasta_data = fetch_fasta_data(url)\n",
    "\n",
    "if fasta_data:\n",
    "    protein_ids = parse_fasta(fasta_data)\n",
    "    print(protein_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50c58e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "protein_ids_250 = protein_ids[:250]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899521ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_primary_structure(uniprot_id):\n",
    "    url_sequence = f'https://www.uniprot.org/uniprot/{uniprot_id}.fasta'\n",
    "    fasta_data = requests.get(url_sequence).text\n",
    "    sequence_lines = fasta_data.split('\\n')[1:]  # get only the sequence\n",
    "    sequence = ''.join(sequence_lines)\n",
    "    return sequence\n",
    "\n",
    "def get_secondary_structure(uniprot_id):\n",
    "    url_xml = f'https://www.uniprot.org/uniprot/{uniprot_id}.xml'\n",
    "    xml_data = requests.get(url_xml).text\n",
    "    i = ET.fromstring(xml_data)\n",
    "\n",
    "    \n",
    "    secondary_structure_annotations = []\n",
    "    for feature in i.findall('.//{http://uniprot.org/uniprot}feature'):\n",
    "        feature_type = feature.attrib.get('type', '')\n",
    "        if feature_type in ['helix', 'strand', 'turn']:\n",
    "            location = feature.find('{http://uniprot.org/uniprot}location')\n",
    "            start = int(location.find('{http://uniprot.org/uniprot}begin').attrib['position'])\n",
    "            end = int(location.find('{http://uniprot.org/uniprot}end').attrib['position'])\n",
    "            if feature_type == 'helix':\n",
    "                secondary_structure_annotations.extend(['H'] * (end - start + 1))\n",
    "            elif feature_type == 'strand':\n",
    "                secondary_structure_annotations.extend(['B'] * (end - start + 1))\n",
    "            elif feature_type == 'turn':\n",
    "                secondary_structure_annotations.extend(['T'] * (end - start + 1))\n",
    "\n",
    "    return ''.join(secondary_structure_annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d6cced",
   "metadata": {},
   "outputs": [],
   "source": [
    "prot_primary_seq = []\n",
    "for i in protein_ids_250:\n",
    "    a = get_primary_structure(i)\n",
    "    prot_primary_seq.append(a)\n",
    "prot_primary_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d8ea37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c15ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "prot_secondary_seq = []\n",
    "for i in protein_ids_250:\n",
    "    b = get_secondary_structure(i)\n",
    "    prot_secondary_seq.append(b)\n",
    "prot_secondary_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cbc717a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0bb9788",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../Final/data.csv')\n",
    "prot_primary_seq = df.iloc[:, 0].tolist()\n",
    "\n",
    "\n",
    "prot_secondary_seq = df.iloc[:, 1].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d00ae19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67397761",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c0de72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "96fc9d1c",
   "metadata": {},
   "source": [
    "# Encodding and padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce0cf39d",
   "metadata": {},
   "outputs": [],
   "source": [
    "properties_dict =  {\n",
    "    'A': [1.28, 0.05, 1.00, 0.31, 6.11, 0.42, 0.23], # [streric parameter, polarizability,volume,hydrophobicity,isoelectric point,H prob.,B prob.]\n",
    "    'G': [0.00, 0.00, 0.00, 0.00, 6.07, 0.13, 0.15],\n",
    "    'V': [3.67, 0.14, 3.00, 1.22, 6.02, 0.27, 0.49],\n",
    "    'L': [2.59, 0.19, 4.00, 1.70, 6.04, 0.39, 0.31],\n",
    "    'I': [4.19, 0.19, 4.00, 1.80, 6.04, 0.30, 0.45],\n",
    "    'F': [2.94, 0.29, 5.89, 1.79, 5.67, 0.30, 0.38],\n",
    "    'Y': [2.94, 0.30, 6.47, 0.96, 5.66, 0.25, 0.41],\n",
    "    'W': [3.21, 0.41, 8.08, 2.25, 5.94, 0.32, 0.42],\n",
    "    'T': [3.03, 0.11, 2.60, 0.26, 5.60, 0.21, 0.36],\n",
    "    'S': [1.31, 0.06, 1.60, -0.04, 5.70, 0.20, 0.28],\n",
    "    'R': [2.34, 0.29, 6.13, -1.01, 10.74, 0.36, 0.25],\n",
    "    'K': [1.89, 0.22, 4.77, -0.99, 9.99, 0.32, 0.27],\n",
    "    'H': [2.99, 0.23, 4.66, 0.13, 7.69, 0.27, 0.30],\n",
    "    'D': [1.60, 0.11, 2.78, -0.77, 2.95, 0.25, 0.20],\n",
    "    'E': [1.56, 0.15, 3.78, -0.64, 3.09, 0.42, 0.21],\n",
    "    'N': [1.60, 0.13, 2.95, -0.60, 6.52, 0.21, 0.22],\n",
    "    'Q': [1.56, 0.18, 3.95, -0.22, 5.65, 0.36, 0.25],\n",
    "    'M': [2.35, 0.22, 4.43, 1.23, 5.71, 0.38, 0.32],\n",
    "    'P': [2.67, 0.00, 2.72, 0.72, 6.80, 0.13, 0.34],\n",
    "    'C': [1.77, 0.13, 2.43, 1.54, 6.35, 0.17, 0.41]\n",
    "}\n",
    "\n",
    "def encode_with_properties(prot_primary_seq, properties_dict):\n",
    "    encoded_seqs = []\n",
    "    for sequence in prot_primary_seq:\n",
    "        encoded_seq = []\n",
    "        for aa in sequence:\n",
    "            if aa in properties_dict:\n",
    "                encoded_seq.append(properties_dict[aa])\n",
    "            else:\n",
    "                encoded_seq.append([0.0] * len(next(iter(properties_dict.values()))))  # Default to zeros if unknown\n",
    "        encoded_seqs.append(encoded_seq)\n",
    "    \n",
    "    return (encoded_seqs)\n",
    "\n",
    "def one_hot_encode2(prot_secondary_seq):\n",
    "    encoded_seqs2 = []\n",
    "    secondary_structure = ['H', 'B', 'T']  \n",
    "    ss_to_index = {ss: i for i, ss in enumerate(secondary_structure)}\n",
    "    \n",
    "    for sequence in prot_secondary_seq:\n",
    "        encoded_seq2 = np.zeros((len(sequence), len(secondary_structure)))\n",
    "        \n",
    "        # go over each secondary structure symbol in the sequence\n",
    "        for i, ss in enumerate(sequence):\n",
    "            if ss in ss_to_index:\n",
    "                index = ss_to_index[ss]\n",
    "                encoded_seq2[i, index] = 1\n",
    "        \n",
    "        encoded_seqs2.append(encoded_seq2)\n",
    "\n",
    "    return encoded_seqs2\n",
    "\n",
    "\n",
    "encoded_primary = encode_with_properties(prot_primary_seq, properties_dict)\n",
    "\n",
    "\n",
    "encoded_secondary = one_hot_encode2(prot_secondary_seq)\n",
    "\n",
    "#primary padding\n",
    "max_length = max(len(seq) for seq in encoded_primary)\n",
    "padded_primary_sequences = pad_sequences(encoded_primary, maxlen=max_length, padding='post',dtype='float32')\n",
    "\n",
    "#secondary padding; keep the same lenght of encoded_primary so that it fits in the model\n",
    "\n",
    "padded_secondary_sequences = pad_sequences(encoded_secondary, maxlen=max_length, padding='post')\n",
    "\n",
    "print(padded_primary_sequences.shape)\n",
    "print(padded_secondary_sequences.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63ed949",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = padded_primary_sequences\n",
    "y = padded_secondary_sequences\n",
    "\n",
    "\n",
    "train_size = 0.7  # 70% of the data for training\n",
    "val_size = 0.15   # 15% of the data for validation\n",
    "test_size = 0.15  # 15% of the data for testing\n",
    "\n",
    "# split the data into training, validation, and test set\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=test_size, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=val_size/(1-test_size), random_state=42)\n",
    "\n",
    "#  verify the shapes of our sets\n",
    "print(\"Training set shapes:\", X_train.shape, y_train.shape)\n",
    "print(\"Validation set shapes:\", X_val.shape, y_val.shape)\n",
    "print(\"Test set shapes:\", X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c137f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from article with input layer and embbeding layer\n",
    "\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(max_length,)),\n",
    "    tf.keras.layers.Embedding(input_dim=21, output_dim=128),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Conv1D(filters=64, kernel_size=3, activation='relu', padding='same'),\n",
    "    tf.keras.layers.Conv1D(filters=128, kernel_size=3, activation='relu', padding='same'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(units=128, return_sequences=True)),\n",
    "    tf.keras.layers.GaussianNoise(0.1),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model2 = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=10, batch_size=32)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f'Test Loss: {loss}, Test Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2739d2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8deba094",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(model2.history['loss'])\n",
    "plt.plot(model2.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "\n",
    "# Plot training & validation accuracy values\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(model2.history['accuracy'])\n",
    "plt.plot(model2.history['val_accuracy'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a4b385",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fafd4e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa227c3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9fd46a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d1dd12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e165754",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e630e18b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b73f50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e9ba49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eadb971",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c41f7d27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a58703f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e849aa24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b92ac1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d25ffe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "664b1582",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c4215f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Define the model (from article)\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Conv1D(filters=64, kernel_size=3, activation='relu', padding='same'),\n",
    "    tf.keras.layers.Conv1D(filters=128, kernel_size=3, activation='relu', padding='same'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(units=128, return_sequences=True)),\n",
    "    tf.keras.layers.GaussianNoise(0.1),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model.fit(X_train, y_train, \n",
    "                    validation_data=(X_val, y_val), epochs=50, batch_size=32)\n",
    "\n",
    "\n",
    "loss, accuracy = model.evaluate(X_test, y_test_one_hot)\n",
    "print(f'Test Loss: {loss}, Test Accuracy: {accuracy}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6618cac6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3dacbdd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
